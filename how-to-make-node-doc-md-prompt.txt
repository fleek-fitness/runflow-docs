I've build a codebase for a workflow automation tool, which is run by connection of the nodes that does some jobs. Your task is to create a documentation of the node, when given the code of the node. I'll provide you with some example documentations. (However, you have to write in Korean) Guideline: - property and input must be combined into one as input, b/c either one will be used. - ignore the isLoop or execute_by_loop method. ignore the return types for the loop mode. such as the nested list - the name of the node must be in English - you should write about "Node 입력", "Node 출력", "기능", "언제 사용할지?" - write down in markdown format - ignore the versions such as V1, V2 etc. - this is a documentation for the users. do not go into too much technical details. such as what API it uses, concurrency, async(비동기) and so on. - however, use the keys of the input and output model directly - do not mention or talk about, 동시성(concurrency), 비동기(async) ex1) Ask AI - Single Value ​ Node Inputs The node has several inputs that allow you to customize how it operates: context: (Optional) This is additional information you can provide to the AI to help it understand the situation or scenario in which it needs to generate a value. Think of it as giving the AI some context or background. value name: This is the specific name of the value you want the AI to generate. For example, if you’re looking for a rating, you might name this customer satisfaction rating. value type: Here you specify what sort of value you expect the AI to generate. It could be text, a number, or a boolean (true/false). value description: In this input, you explain what the value represents and how the AI should decide what to generate. Giving clear criteria and examples can help the AI understand what you’re looking for. For instance, if you’re asking for a rating, describe the scale and what each point on that scale means. ​ Node Output The node provides a single output: value: This is the direct, no-nonsense result produced by the AI. It corresponds to the value name you defined and is presented in the value type format you specified without any extra information or fluff. ​ Node Functionality The “Ask AI - Single Value” node is designed to interact with an AI model and enforce that the AI returns a very specific piece of information or a single value, as per your request. It handles the communication with the AI, making sure the AI has all the necessary context and information to deliver the precise value you need. ​ When To Use Consider using this node when you need to extract a particular piece of information from a large dataset or require the AI to focus on a single piece of data rather than providing broader, more elaborate responses. This can be particularly useful in scenarios where precision and specificity are more valuable than breadth of information—for instance, generating a specific rating, a yes/no answer to a compliance question, or perhaps a single data point from customer feedback. ex2) Combine Text This node is designed to merge two separate pieces of text following a specific pattern defined by the user. It is versatile, making it suitable for various scenarios where text combination is needed. ​ Node Inputs The following are the inputs that the Combine Text node accepts: format: A template that indicates how to combine the two inputs. The placeholders {input1} and {input2} are used to symbolize where the first and second inputs will be included in the combined result. Example: 'This is input 1: {input1} This is input 2: {input2}'. input1: This represents the first input that will be part of the combination. It is okay if this input is not provided. input2: Similar to input1, this is the second input that you want to include in the combination. This input is also optional. These inputs allow the user to instruct the node on what text should be combined and the pattern that should guide the combination. ​ Node Output The node produces a single output: combined text: This is the final text that results from merging input1 and input2 according to the formatted text provided. If the operation is successful, this output will contain the combined text ready for use or further processing. ​ Node Functionality The Combine Text node takes the provided inputs and merges them using the specified format. If a user does not supply one of the inputs, the node will still function by simply not including the missing part in the combined result. ​ When To Use This node is particularly useful in situations where you need to standardize the combination of different pieces of text, such as generating personalized messages, formatting data for reports, or any process where textual information from separate sources needs to be brought together in a consistent manner. It can be utilized in automation workflows where dynamic text construction is required, offering a straightforward way to create structured text without needing manual effort. code: from loguru import logger import asyncio from tenacity import retry, stop_after_attempt, wait_exponential from typing import Union, List from pydantic import BaseModel, Field, ConfigDict from app.flow.core import Node from app.schemas.core import RawNodeSchema from app.schemas.enums import NodeType, LLMModelType from app.utils.redis import CancellationToken import litellm from app.utils.redis import CancellationToken from app.utils.error import FlowCancelledException litellm.set_verbose = False class LLMCallerInputV1(BaseModel): prompt: Union[str, List[str]] | None = Field(alias="prompt", default=None) context: Union[str, List[str]] | None = Field(alias="context", default=None) model_config = ConfigDict(extra="forbid") class LLMCallerPropertyV1(BaseModel): prompt: str = Field(alias="prompt") context: str | None = Field(alias="context", default=None) is_loop: bool = Field(alias="isLoop", default=False) class LLMCallerOutputV1(BaseModel): response: Union[str, List[str]] | None = Field(alias="response", default=None) model_config = ConfigDict(extra="forbid") class LLMCallerConfigV1(BaseModel): input: LLMCallerInputV1 property: LLMCallerPropertyV1 output: LLMCallerOutputV1 class LLMCallerV1(Node[LLMCallerInputV1, LLMCallerPropertyV1, LLMCallerOutputV1]): type = NodeType.LLM_CALLER_V1 max_concurrency = 5 @classmethod def create( cls, id: str, raw_node: RawNodeSchema, ): config = LLMCallerConfigV1( input=LLMCallerInputV1(), property=LLMCallerPropertyV1( **{key: value.value for key, value in raw_node.data.property.items()} ), output=LLMCallerOutputV1(), ) return cls( id=id, config=config, raw_node=raw_node, ) async def execute(self) -> LLMCallerOutputV1: self.semaphore = asyncio.Semaphore(self.max_concurrency) if self.property.is_loop: result = await self.execute_by_loop() else: result = await self.execute_by_single() return result async def execute_by_loop(self) -> LLMCallerOutputV1: prompts = self.input.prompt or [self.property.prompt] contexts = self.input.context or [None] * len(prompts) self.tasks = [ asyncio.create_task( self.call_llm( prompt=prompt, context=context, semaphore=self.semaphore, cancellation_token=self.cancellation_token, ) ) for prompt, context in zip(prompts, contexts) ] responses: List[str] = await asyncio.gather(*self.tasks) return LLMCallerOutputV1(response=responses) async def execute_by_single(self) -> LLMCallerOutputV1: prompt = self.input.prompt or self.property.prompt context = self.input.context or self.property.context task = asyncio.create_task( self.call_llm( prompt=prompt, context=context, semaphore=self.semaphore, cancellation_token=self.cancellation_token, ) ) response: str = await task return LLMCallerOutputV1(response=response) @staticmethod @retry( stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10), ) async def call_llm( prompt: str, context: str | None, semaphore: asyncio.Semaphore, cancellation_token: CancellationToken, ) -> str: async with semaphore: if await cancellation_token.is_cancelled(): raise FlowCancelledException("Flow execution was cancelled.") system_message = "You are a helpful assistant." user_prompt = f"""Given the context: <context> {context} </context> Respond to the user's query: {prompt}""" response = await litellm.acompletion( model=LLMModelType.GPT_4O.value, messages=[ {"role": "system", "content": system_message}, {"role": "user", "content": user_prompt}, ], ) return response.choices[0].message.content.strip() if __name__ == "__main__": semaphore = asyncio.Semaphore(1) result = asyncio.run( LLMCallerV1.call_llm( prompt="What is the capital of France?", context=None, semaphore=semaphore, ) ) print(result)